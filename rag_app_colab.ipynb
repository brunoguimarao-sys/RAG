{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "from werkzeug.utils import secure_filename\n",
    "import threading\n",
    "import fitz  # PyMuPDF\n",
    "from docx2python import docx2python\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import sys\n",
    "import subprocess\n",
    "import atexit\n",
    "import torch\n",
    "\n",
    "# Imports for Colab\n",
    "from flask_ngrok import run_with_ngrok # For running Flask with ngrok in Colab\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: CONFIGURAÇÃO DE LOGGING ---\n",
    "# ==============================================================================\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
    "# File handler para escrever logs em um arquivo\n",
    "file_handler = logging.FileHandler('rag_app.log', mode='w', encoding='utf-8')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "# Stream handler para exibir logs no console\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: CAMINHOS E PARÂMETROS GLOBAIS (AJUSTADOS PARA COLAB) ---\n",
    "# ==============================================================================\n",
    "UPLOAD_FOLDER = 'uploads_colab' # Ajustado para Colab\n",
    "DOCUMENTS_DIR = os.getenv(\"DOCUMENTS_DIR\", \"/content/Documentos\") # Ajustado para Colab, crie esta pasta e adicione seus arquivos\n",
    "CHROMA_DB_PATH = os.getenv(\"CHROMA_DB_PATH\", \"chroma_db_colab\") # Ajustado para Colab\n",
    "CHROMA_COLLECTION_NAME = \"document_chunks\"\n",
    "DB_PATH = \"audit_colab.db\"  # Banco de dados SQLite para auditoria, ajustado para Colab\n",
    "TESSERACT_CMD_PATH = os.getenv(\"TESSERACT_CMD_PATH\", \"/usr/bin/tesseract\") # Caminho padrão do Tesseract no Colab após instalação\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: PARÂMETROS DO SERVIDOR LLAMA (AJUSTADOS PARA COLAB) ---\n",
    "# ==============================================================================\n",
    "LLAMA_MODEL_PATH = os.getenv(\"LLAMA_MODEL_PATH\", \"/content/amoral-gemma3-4B-v2-qat.Q4_K_S.gguf\") # Ajustado para Colab, adicione seu modelo aqui\n",
    "LLAMA_SERVER_URL = os.getenv(\"LLAMA_SERVER_URL\", \"http://localhost:8080/completion\") # Mantenha ou ajuste se seu servidor Llama estiver em outro URL\n",
    "LLAMA_HOST = \"127.0.0.1\"\n",
    "LLAMA_PORT = 8080\n",
    "LLAMA_NGL = -1  # Número de camadas para descarregar na GPU (-1 para GPU se disponível, 0 para CPU)\n",
    "LLAMA_THREADS = 10\n",
    "LLAMA_THREADS_BATCH = 10\n",
    "LLAMA_BATCH_SIZE = 512\n",
    "LLAMA_CONTEXT_SIZE = 4096\n",
    "LLAMA_TEMP = 0.7\n",
    "LLAMA_TOP_K = 40\n",
    "LLAMA_TOP_P = 0.95\n",
    "LLAMA_REPEAT_PENALTY = 1.1\n",
    "llama_server_process = None  # Variável global para o processo do servidor Llama\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: PARÂMETROS DE EMBEDDING E INDEXAÇÃO ---\n",
    "# ==============================================================================\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # Modelo de embedding da Sentence Transformers\n",
    "CHUNK_SIZE = 500  # Tamanho dos chunks de texto\n",
    "CHUNK_OVERLAP = 50  # Sobreposição entre chunks\n",
    "TOP_K = 3  # Número de chunks mais relevantes a serem recuperados\n",
    "MAX_WORKERS = os.cpu_count() or 4  # Número de workers para processamento paralelo\n",
    "SUPPORTED_EXTENSIONS = {'.pdf', '.docx', '.txt', '.jpg', '.jpeg', '.png'}\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: INICIALIZAÇÃO DE APLICAÇÃO E MODELOS ---\n",
    "# ==============================================================================\n",
    "app = Flask(__name__, static_folder=\"/content/static\") # Ajustado para Colab, crie /content/static e adicione index.html se necessário\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "CORS(app)  # Habilita CORS para a aplicação Flask\n",
    "run_with_ngrok(app) # Adicionado para expor o Flask app via ngrok em Colab\n",
    "\n",
    "# Define o dispositivo para o modelo de embedding (GPU se disponível, senão CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "logger.info(f\"Usando dispositivo para embedding: {device.upper()}\")\n",
    "\n",
    "logger.info(\"Carregando o modelo de embedding...\")\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=device)\n",
    "logger.info(\"Modelo de embedding carregado com sucesso.\")\n",
    "\n",
    "# Configura o caminho do Tesseract OCR, se existir\n",
    "if os.path.exists(TESSERACT_CMD_PATH):\n",
    "    pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD_PATH\n",
    "else:\n",
    "    logger.error(f\"Caminho do Tesseract OCR não encontrado: {TESSERACT_CMD_PATH}. A extração de texto de imagens pode falhar.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: GERENCIAMENTO DO SERVIDOR LLAMA (AJUSTADO PARA COLAB) ---\n",
    "# Em Colab, o gerenciamento direto de processos do servidor Llama como no script original é complexo.\n",
    "# Recomenda-se executar o servidor Llama (ex: llama.cpp server) em um terminal separado ou outra instância Colab\n",
    "# e garantir que LLAMA_SERVER_URL aponte para ele.\n",
    "# As funções start_llama_server e shutdown_llama_server foram comentadas.\n",
    "# ==============================================================================\n",
    "def is_server_running():\n",
    "    \"\"\"Verifica se o servidor Llama está em execução (mantido para checagem).\"\"\"\n",
    "    try:\n",
    "        requests.get(LLAMA_SERVER_URL.replace(\"/completion\", \"/\"), timeout=2) # Tenta acessar a raiz do servidor Llama\n",
    "        logger.info(f\"Servidor Llama parece estar respondendo em {LLAMA_SERVER_URL}.\")\n",
    "        return True\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.warning(f\"Servidor Llama não parece estar respondendo em {LLAMA_SERVER_URL}.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao verificar o servidor Llama: {e}\")\n",
    "        return False\n",
    "\n",
    "# def start_llama_server():\n",
    "#     \"\"\"Inicia o servidor Llama em um processo separado (COMENTADO PARA COLAB).\"\"\"\n",
    "#     global llama_server_process\n",
    "#     if is_server_running():\n",
    "#         logger.info(\"Servidor Llama já está em execução.\")\n",
    "#         return\n",
    "# \n",
    "#     if not os.path.exists(LLAMA_MODEL_PATH):\n",
    "#         logger.critical(f\"CRÍTICO: Modelo do LLM não encontrado em: '{LLAMA_MODEL_PATH}'. O servidor não pode ser iniciado.\")\n",
    "#         # sys.exit(1) # Removido para Colab\n",
    "#         logger.error(\"Saindo devido à ausência do modelo LLM.\")\n",
    "#         return # Retorna para não tentar iniciar o servidor\n",
    "# \n",
    "#     command = [\n",
    "#         \"llama-server\", \"-m\", LLAMA_MODEL_PATH, \"-c\", str(LLAMA_CONTEXT_SIZE),\n",
    "#         \"-t\", str(LLAMA_THREADS), \"-tb\", str(LLAMA_THREADS_BATCH), \"-b\", str(LLAMA_BATCH_SIZE),\n",
    "#         \"--host\", LLAMA_HOST, \"--port\", str(LLAMA_PORT), \"-ngl\", str(LLAMA_NGL),\n",
    "#         \"--temp\", str(LLAMA_TEMP), \"--top-k\", str(LLAMA_TOP_K),\n",
    "#         \"--top-p\", str(LLAMA_TOP_P), \"--repeat-penalty\", str(LLAMA_REPEAT_PENALTY)\n",
    "#     ]\n",
    "#     logger.info(f\"Comando para iniciar o servidor Llama (NÃO EXECUTADO EM COLAB): {' '.join(command)}\")\n",
    "#     logger.warning(\"A inicialização automática do servidor Llama está desabilitada em Colab. Execute-o separadamente.\")\n",
    "#     # Lógica de subprocess.Popen e verificação removida para Colab\n",
    "\n",
    "# def shutdown_llama_server():\n",
    "#     \"\"\"Encerra o processo do servidor Llama (COMENTADO PARA COLAB).\"\"\"\n",
    "#     global llama_server_process\n",
    "#     if llama_server_process and llama_server_process.poll() is None:\n",
    "#         logger.info(\"Encerrando o servidor Llama (simulado em Colab)...\")\n",
    "#         # llama_server_process.terminate() # Lógica de subprocess removida\n",
    "#         # try:\n",
    "#         #     llama_server_process.wait(timeout=10)\n",
    "#         #     logger.info(\"Servidor Llama encerrado com sucesso (simulado).\")\n",
    "#         # except subprocess.TimeoutExpired:\n",
    "#         #     logger.warning(\"Servidor Llama não encerrou a tempo (simulado).\")\n",
    "#         #     llama_server_process.kill()\n",
    "#     logger.warning(\"O encerramento automático do servidor Llama está desabilitado em Colab.\")\n",
    "\n",
    "# atexit.register(shutdown_llama_server) # Removido para Colab, pois o servidor não é gerenciado aqui\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: BANCO DE DADOS (CHROMA DB E SQLITE) E AUDITORIA ---\n",
    "# ==============================================================================\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH) # Cliente persistente para o ChromaDB\n",
    "collection = None # Variável global para a coleção do ChromaDB\n",
    "\n",
    "def get_or_create_collection():\n",
    "    \"\"\"Obtém ou cria a coleção no ChromaDB com uma função de embedding customizada.\"\"\"\n",
    "    global collection\n",
    "    if collection is None:\n",
    "        class CustomEmbeddingFunction(chromadb.api.types.EmbeddingFunction):\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "            def __call__(self, texts: list[str]) -> list[list[float]]:\n",
    "                # Gera embeddings em lotes para eficiência\n",
    "                return embedding_model.encode(texts, batch_size=128, show_progress_bar=False).tolist()\n",
    "\n",
    "        collection = client.get_or_create_collection(\n",
    "            name=CHROMA_COLLECTION_NAME,\n",
    "            embedding_function=CustomEmbeddingFunction() # Usa a função de embedding customizada\n",
    "        )\n",
    "        logger.info(f\"Coleção '{CHROMA_COLLECTION_NAME}' carregada/criada no ChromaDB.\")\n",
    "    return collection\n",
    "\n",
    "def init_db():\n",
    "    \"\"\"Inicializa o banco de dados SQLite para auditoria.\"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS auditoria (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    pergunta TEXT,\n",
    "                    resposta TEXT,\n",
    "                    contexto TEXT,\n",
    "                    arquivos_utilizados TEXT,\n",
    "                    data TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\")\n",
    "        logger.info(\"Banco de dados de auditoria inicializado com sucesso.\")\n",
    "    except sqlite3.Error as e:\n",
    "        logger.error(f\"Erro ao inicializar o banco de dados de auditoria: {e}\")\n",
    "\n",
    "def registrar_auditoria(pergunta: str, resposta: str, contexto: str, arquivos: list[str]):\n",
    "    \"\"\"Registra uma entrada de auditoria no banco de dados SQLite.\"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            conn.execute(\n",
    "                \"INSERT INTO auditoria (pergunta, resposta, contexto, arquivos_utilizados) VALUES (?,?,?,?)\",\n",
    "                (pergunta, resposta, contexto, '; '.join(arquivos)) # Concatena a lista de arquivos em uma string\n",
    "            )\n",
    "            conn.commit()\n",
    "        logger.info(f\"Auditoria registrada para a pergunta: '{pergunta[:50]}...'\" )\n",
    "    except sqlite3.Error as e:\n",
    "        logger.error(f\"Erro ao registrar auditoria: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: PROCESSAMENTO E EXTRAÇÃO DE TEXTO DE ARQUIVOS ---\n",
    "# ==============================================================================\n",
    "def encontrar_arquivos_recursivamente(diretorio: str) -> list[Path]:\n",
    "    \"\"\"Encontra todos os arquivos com extensões suportadas em um diretório recursivamente.\"\"\"\n",
    "    arquivos_encontrados = []\n",
    "    diretorio_path = Path(diretorio)\n",
    "    if not diretorio_path.exists() or not diretorio_path.is_dir():\n",
    "        logger.error(f\"Diretório de documentos não existe ou não é um diretório: {diretorio}\")\n",
    "        return arquivos_encontrados\n",
    "    \n",
    "    logger.info(f\"Buscando arquivos recursivamente em: {diretorio}\")\n",
    "    for extensao in SUPPORTED_EXTENSIONS:\n",
    "        pattern = f\"**/*{extensao}\" # Padrão para encontrar arquivos com a extensão\n",
    "        arquivos = list(diretorio_path.rglob(pattern))\n",
    "        arquivos_encontrados.extend(arquivos)\n",
    "        if arquivos:\n",
    "            logger.info(f\"Encontrados {len(arquivos)} arquivos com extensão {extensao}\")\n",
    "            \n",
    "    arquivos_encontrados = sorted(list(set(arquivos_encontrados))) # Remove duplicatas e ordena\n",
    "    logger.info(f\"Total de arquivos únicos encontrados: {len(arquivos_encontrados)}\")\n",
    "    return arquivos_encontrados\n",
    "\n",
    "# Funções de pré-processamento de imagem para OCR\n",
    "def get_grayscale(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Converte uma imagem para escala de cinza.\"\"\"\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def deskew(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Corrige a inclinação (skew) de uma imagem.\"\"\"\n",
    "    if len(image.shape) == 3: image = get_grayscale(image) # Converte para cinza se necessário\n",
    "    coords = np.column_stack(np.where(image > 0)) # Encontra coordenadas de pixels não pretos\n",
    "    if len(coords) < 2: return image # Retorna a imagem original se não houver pixels suficientes\n",
    "    \n",
    "    angle = cv2.minAreaRect(coords)[-1] # Calcula o ângulo de inclinação\n",
    "    if angle < -45: angle = -(90 + angle)\n",
    "    else: angle = -angle\n",
    "    \n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0) # Cria a matriz de rotação\n",
    "    # Aplica a rotação para corrigir a inclinação\n",
    "    return cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "def thresholding(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Aplica binarização (thresholding) à imagem.\"\"\"\n",
    "    if len(image.shape) == 3: image = get_grayscale(image) # Converte para cinza se necessário\n",
    "    # Usa o método de Otsu para binarização automática\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "def extrair_texto(path: Path) -> str:\n",
    "    \"\"\"Extrai texto de diferentes tipos de arquivo (PDF, DOCX, TXT, Imagens).\"\"\"\n",
    "    texto_completo = \"\"\n",
    "    try:\n",
    "        suffix = path.suffix.lower() # Obtém a extensão do arquivo em minúsculas\n",
    "        \n",
    "        if suffix == \".pdf\":\n",
    "            with fitz.open(path) as doc:\n",
    "                for page_num, page in enumerate(doc):\n",
    "                    text_native = page.get_text().strip()\n",
    "                    if text_native: # Tenta extrair texto nativo primeiro\n",
    "                        texto_completo += text_native + \"\\n\"\n",
    "                    else: # Se não houver texto nativo, tenta OCR\n",
    "                        logger.info(f\"Página {page_num+1} do PDF '{path.name}' não contém texto nativo, tentando OCR.\")\n",
    "                        pix = page.get_pixmap(dpi=300) # Renderiza a página como imagem\n",
    "                        img_np = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.h, pix.w, pix.n)\n",
    "                        if pix.n == 4: img_np = cv2.cvtColor(img_np, cv2.COLOR_RGBA2BGR) # Converte RGBA para BGR\n",
    "                        \n",
    "                        # Pré-processamento da imagem para OCR\n",
    "                        img_processed = get_grayscale(img_np)\n",
    "                        img_processed = thresholding(img_processed)\n",
    "                        img_processed = deskew(img_processed)\n",
    "                        \n",
    "                        texto_ocr = pytesseract.image_to_string(Image.fromarray(img_processed), lang='por+eng') # OCR com Português e Inglês\n",
    "                        if texto_ocr.strip():\n",
    "                            logger.info(f\"Texto extraído via OCR da página {page_num+1} do PDF '{path.name}'.\")\n",
    "                            texto_completo += texto_ocr + \"\\n\"\n",
    "                        else:\n",
    "                            logger.warning(f\"Nenhum texto extraído (OCR) da página {page_num+1} do PDF '{path.name}'.\")\n",
    "        elif suffix == \".docx\":\n",
    "            with docx2python(str(path)) as docx_content:\n",
    "                texto_completo = docx_content.text\n",
    "        elif suffix == \".txt\":\n",
    "            # Tenta diferentes encodings comuns para arquivos de texto\n",
    "            for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
    "                try:\n",
    "                    texto_completo = path.read_text(encoding=encoding)\n",
    "                    logger.info(f\"Texto extraído de '{path.name}' com encoding '{encoding}'.\")\n",
    "                    break \n",
    "                except UnicodeDecodeError:\n",
    "                    if encoding == 'cp1252': # Se falhar no último encoding, loga o erro\n",
    "                        logger.warning(f\"Não foi possível decodificar '{path.name}' com encodings testados.\")\n",
    "                    continue\n",
    "        elif suffix in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "            img = cv2.imread(str(path))\n",
    "            if img is None: \n",
    "                logger.error(f\"Não foi possível carregar a imagem: {path.name}\")\n",
    "                raise ValueError(\"Não foi possível carregar a imagem\")\n",
    "            \n",
    "            # Pré-processamento da imagem para OCR\n",
    "            img_processed = get_grayscale(img)\n",
    "            img_processed = thresholding(img_processed)\n",
    "            img_processed = deskew(img_processed)\n",
    "            \n",
    "            texto_completo = pytesseract.image_to_string(Image.fromarray(img_processed), lang='por+eng') # OCR\n",
    "            if not texto_completo.strip():\n",
    "                logger.warning(f\"Nenhum texto extraído (OCR) da imagem '{path.name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha ao extrair texto de {path.name}: {e}\", exc_info=True) # Loga a exceção com traceback\n",
    "        return \"\" # Retorna string vazia em caso de erro\n",
    "    \n",
    "    return texto_completo.strip()\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: INDEXAÇÃO E GERAÇÃO DE CHUNKS ---\n",
    "# ==============================================================================\n",
    "def quebrar_em_chunks(texto: str, arquivo_path: str) -> tuple[list[str], list[dict], list[str]]:\n",
    "    \"\"\"Quebra o texto em chunks menores e gera metadados e IDs para cada chunk.\"\"\"\n",
    "    if not texto.strip(): return [], [], [] # Retorna listas vazias se o texto estiver vazio\n",
    "    \n",
    "    # Usa RecursiveCharacterTextSplitter para quebrar o texto\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, length_function=len)\n",
    "    chunks_text = splitter.split_text(texto)\n",
    "    \n",
    "    metadados_list, ids_list = [], []\n",
    "    # Normaliza o caminho do arquivo para criar IDs únicos e consistentes\n",
    "    normalized_file_path = str(Path(arquivo_path)).encode('utf-8').hex() \n",
    "    \n",
    "    for i, chunk_content in enumerate(chunks_text):\n",
    "        unique_chunk_id = f\"{normalized_file_path}_chunk_{i}\" # Cria um ID único para o chunk\n",
    "        ids_list.append(unique_chunk_id)\n",
    "        metadados_list.append({\n",
    "            'arquivo_path': str(arquivo_path), \n",
    "            'arquivo_nome': Path(arquivo_path).name, \n",
    "            'chunk_id': i\n",
    "        })\n",
    "    return chunks_text, metadados_list, ids_list\n",
    "\n",
    "def processar_arquivo_para_indexacao(arquivo_path: Path) -> tuple[list[str] | None, list[dict] | None, list[str] | None, bool]:\n",
    "    \"\"\"Processa um único arquivo: extrai texto, quebra em chunks e prepara para indexação.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Processando arquivo para indexação: {arquivo_path.name}\")\n",
    "        texto = extrair_texto(arquivo_path)\n",
    "        if not texto:\n",
    "            logger.warning(f\"Nenhum texto válido extraído de: {arquivo_path.name}. Arquivo não será indexado.\")\n",
    "            return None, None, None, False\n",
    "            \n",
    "        chunks, metadatas, ids = quebrar_em_chunks(texto, str(arquivo_path))\n",
    "        if not chunks:\n",
    "            logger.warning(f\"Texto extraído de '{arquivo_path.name}' não gerou chunks válidos. Arquivo não será indexado.\")\n",
    "            return None, None, None, False\n",
    "            \n",
    "        logger.info(f\"[{len(chunks)}] chunks gerados com sucesso de {arquivo_path.name}\")\n",
    "        return chunks, metadatas, ids, True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro crítico ao processar {arquivo_path.name} para indexação: {e}\", exc_info=True)\n",
    "        return None, None, None, False\n",
    "\n",
    "def indexar_documentos(diretorio_documentos: str = DOCUMENTS_DIR) -> bool:\n",
    "    \"\"\"Indexa todos os documentos encontrados no diretório especificado.\"\"\"\n",
    "    logger.info(f\"Iniciando processo de indexação completa dos documentos em: {diretorio_documentos}\")\n",
    "    arquivos_para_indexar = encontrar_arquivos_recursivamente(diretorio_documentos)\n",
    "    \n",
    "    if not arquivos_para_indexar:\n",
    "        logger.error(\"Nenhum arquivo encontrado para indexação. Verifique o diretório e as extensões suportadas.\")\n",
    "        return False\n",
    "\n",
    "    logger.info(\"Recriando coleção no ChromaDB para garantir uma reindexação completa...\")\n",
    "    try:\n",
    "        client.delete_collection(name=CHROMA_COLLECTION_NAME) # Deleta a coleção existente para evitar duplicatas\n",
    "        logger.info(f\"Coleção antiga '{CHROMA_COLLECTION_NAME}' deletada com sucesso.\")\n",
    "    except Exception as e: # chromadb.errors.CollectionNotFoundError ou similar\n",
    "        logger.warning(f\"Não foi possível deletar a coleção '{CHROMA_COLLECTION_NAME}' (provavelmente não existia): {e}\")\n",
    "    \n",
    "    chroma_collection = get_or_create_collection() # Cria uma nova coleção (ou obtém se já existir, embora deletada acima)\n",
    "    \n",
    "    all_chunks_list, all_metadatas_list, all_ids_list = [], [], []\n",
    "    arquivos_processados_sucesso, arquivos_processados_erro = 0, 0\n",
    "\n",
    "    # Usa ProcessPoolExecutor para processar arquivos em paralelo\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_file = {executor.submit(processar_arquivo_para_indexacao, arq_path): arq_path for arq_path in arquivos_para_indexar}\n",
    "        for future in as_completed(future_to_file): # Processa os resultados à medida que ficam prontos\n",
    "            try:\n",
    "                chunks, metadatas, ids, sucesso = future.result()\n",
    "                if sucesso and chunks: # Verifica se o processamento foi bem-sucedido e gerou chunks\n",
    "                    all_chunks_list.extend(chunks)\n",
    "                    all_metadatas_list.extend(metadatas)\n",
    "                    all_ids_list.extend(ids)\n",
    "                    arquivos_processados_sucesso += 1\n",
    "                else:\n",
    "                    arquivos_processados_erro += 1\n",
    "            except Exception as exc:\n",
    "                arquivo_orig = future_to_file[future]\n",
    "                logger.error(f\"Falha no processamento do arquivo {arquivo_orig.name} durante a indexação em paralelo: {exc}\", exc_info=True)\n",
    "                arquivos_processados_erro += 1\n",
    "                \n",
    "    logger.info(f\"Processamento de arquivos para indexação concluído. Sucesso: {arquivos_processados_sucesso}, Erros: {arquivos_processados_erro}.\")\n",
    "\n",
    "    if not all_chunks_list:\n",
    "        logger.warning(\"Nenhum chunk válido foi gerado para indexação após o processamento de todos os arquivos.\")\n",
    "        return False\n",
    "\n",
    "    logger.info(f\"Adicionando {len(all_chunks_list)} chunks ao ChromaDB em lotes...\")\n",
    "    try:\n",
    "        # Adiciona os chunks ao ChromaDB em lotes para otimizar o desempenho\n",
    "        batch_size_chroma = 500 # Tamanho do lote para adicionar ao ChromaDB\n",
    "        for i in range(0, len(all_chunks_list), batch_size_chroma):\n",
    "            chroma_collection.add(\n",
    "                documents=all_chunks_list[i:i+batch_size_chroma],\n",
    "                metadatas=all_metadatas_list[i:i+batch_size_chroma],\n",
    "                ids=all_ids_list[i:i+batch_size_chroma]\n",
    "            )\n",
    "            logger.info(f\"Lote {i//batch_size_chroma + 1} de { (len(all_chunks_list) + batch_size_chroma -1) // batch_size_chroma } adicionado ao ChromaDB.\")\n",
    "        logger.info(\"Indexação de todos os documentos concluída com sucesso!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha crítica ao adicionar documentos ao ChromaDB: {e}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "def indexar_arquivo_unico(caminho_arquivo: str):\n",
    "    \"\"\"Processa e indexa um único arquivo no ChromaDB. Usado para uploads.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"[INDEXAÇÃO DE ARQUIVO ÚNICO EM BACKGROUND] Iniciando para: {caminho_arquivo}\")\n",
    "        arquivo_path = Path(caminho_arquivo)\n",
    "        \n",
    "        if not arquivo_path.exists():\n",
    "            logger.error(f\"[INDEXAÇÃO EM BACKGROUND] Arquivo não encontrado: {caminho_arquivo}\")\n",
    "            return\n",
    "\n",
    "        chunks, metadatas, ids, sucesso = processar_arquivo_para_indexacao(arquivo_path)\n",
    "\n",
    "        if not sucesso or not chunks:\n",
    "            logger.error(f\"[INDEXAÇÃO EM BACKGROUND] Falha ao processar e gerar chunks para {arquivo_path.name}. O arquivo não será indexado.\")\n",
    "            return\n",
    "\n",
    "        chroma_collection = get_or_create_collection() # Garante que a coleção esteja disponível\n",
    "        # Adiciona os chunks do arquivo único à coleção existente\n",
    "        chroma_collection.add(\n",
    "            documents=chunks,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        logger.info(f\"[INDEXAÇÃO EM BACKGROUND] Sucesso! Adicionados {len(chunks)} chunks de {arquivo_path.name} ao ChromaDB.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[INDEXAÇÃO EM BACKGROUND] Erro crítico ao indexar o arquivo '{caminho_arquivo}': {e}\", exc_info=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: RECUPERAÇÃO DE CONTEXTO E GERAÇÃO DE RESPOSTA (RAG) ---\n",
    "# ==============================================================================\n",
    "def recuperar_contexto(query: str, top_k_chunks: int = TOP_K) -> tuple[str, list[str]]:\n",
    "    \"\"\"Recupera chunks de contexto relevantes do ChromaDB com base na query.\"\"\"\n",
    "    try:\n",
    "        chroma_collection = get_or_create_collection()\n",
    "        if chroma_collection.count() == 0:\n",
    "            logger.warning(\"O índice (ChromaDB) está vazio. Execute a reindexação primeiro ou faça upload de arquivos.\")\n",
    "            return \"\", [] # Retorna contexto vazio e lista de arquivos vazia\n",
    "            \n",
    "        # Realiza a query na coleção do ChromaDB\n",
    "        results = chroma_collection.query(\n",
    "            query_texts=[query], \n",
    "            n_results=top_k_chunks, \n",
    "            include=['documents', 'metadatas'] # Inclui os documentos (chunks) e metadados\n",
    "        )\n",
    "        \n",
    "        chunks_com_fonte, arquivos_utilizados_set = [], set()\n",
    "        if results and results['documents'] and results['documents'][0]:\n",
    "            document_list, metadata_list = results['documents'][0], results['metadatas'][0]\n",
    "            for doc_content, meta_info in zip(document_list, metadata_list):\n",
    "                arquivo_nome_orig = meta_info.get('arquivo_nome', 'Desconhecido')\n",
    "                arquivos_utilizados_set.add(arquivo_nome_orig)\n",
    "                chunks_com_fonte.append(f\"[Fonte: {arquivo_nome_orig}]\\n{doc_content}\") # Adiciona a fonte ao chunk\n",
    "        \n",
    "        contexto_final = \"\\n---\\n\".join(chunks_com_fonte) # Une os chunks com um separador\n",
    "        return contexto_final, list(arquivos_utilizados_set)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha ao recuperar contexto do ChromaDB: {e}\", exc_info=True)\n",
    "        return f\"Erro ao recuperar contexto do ChromaDB: {e}\", [] # Retorna mensagem de erro e lista vazia\n",
    "\n",
    "def gerar_resposta(prompt_completo: str) -> str:\n",
    "    \"\"\"Envia o prompt completo para o servidor Llama e obtém a resposta.\"\"\"\n",
    "    if not is_server_running():\n",
    "        logger.error(\"Servidor Llama não está em execução. Não é possível gerar resposta.\")\n",
    "        return \"Erro: O servidor de IA (Llama) não está disponível no momento. Por favor, tente mais tarde.\"\n",
    "\n",
    "    try:\n",
    "        payload = {\n",
    "            \"prompt\": prompt_completo, \n",
    "            \"n_predict\": 1024, # Aumentado para respostas potencialmente mais longas\n",
    "            \"temperature\": LLAMA_TEMP, \n",
    "            \"top_k\": LLAMA_TOP_K,\n",
    "            \"top_p\": LLAMA_TOP_P,\n",
    "            \"repeat_penalty\": LLAMA_REPEAT_PENALTY,\n",
    "            \"stop\": [\"\\n\\n\", \"Pergunta:\", \"Contexto:\", \"Usuário:\"] # Palavras/frases para parar a geração\n",
    "        }\n",
    "        response = requests.post(LLAMA_SERVER_URL, json=payload, timeout=120) # Timeout aumentado para 2 minutos\n",
    "        response.raise_for_status() # Levanta uma exceção para códigos de erro HTTP (4xx ou 5xx)\n",
    "        \n",
    "        resposta_json = response.json()\n",
    "        # Verifica se a resposta contém 'content' e se não está vazia\n",
    "        if \"content\" in resposta_json and resposta_json[\"content\"].strip():\n",
    "            return resposta_json[\"content\"].strip()\n",
    "        else:\n",
    "            logger.warning(f\"Resposta do Llama não continha 'content' ou estava vazia. Payload: {payload}, Resposta: {resposta_json}\")\n",
    "            return \"O modelo de IA não forneceu uma resposta válida para esta pergunta.\"\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Timeout na comunicação com o servidor Llama em {LLAMA_SERVER_URL}.\")\n",
    "        return f\"Erro: Timeout ao tentar conectar ao servidor de IA. A solicitação demorou muito para responder.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Falha na comunicação com o servidor Llama: {e}\")\n",
    "        return f\"Erro: Não foi possível conectar ao servidor de IA em {LLAMA_SERVER_URL}. Detalhes: {e}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha inesperada ao gerar resposta: {e}\", exc_info=True)\n",
    "        return f\"Erro inesperado ao gerar a resposta: {e}\"\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: ENDPOINTS DA API FLASK ---\n",
    "# ==============================================================================\n",
    "@app.route(\"/\")\n",
    "def frontend():\n",
    "    \"\"\"Serve a página principal do frontend (index.html).\"\"\"\n",
    "    return send_from_directory(\"static\", \"index.html\")\n",
    "\n",
    "@app.route(\"/perguntar\", methods=['POST'])\n",
    "def perguntar_endpoint():\n",
    "    \"\"\"Endpoint para receber perguntas, processá-las e retornar respostas.\"\"\"\n",
    "    try:\n",
    "        # Lida com 'multipart/form-data' para permitir upload de arquivos junto com a pergunta\n",
    "        query = request.form.get(\"pergunta\", \"\").strip()\n",
    "        uploaded_file = request.files.get(\"arquivo\") # 'arquivo' é o nome do campo no form-data\n",
    "        incluir_contexto_na_resposta = request.form.get(\"incluir_contexto\", 'false').lower() == 'true'\n",
    "        \n",
    "        if not query:\n",
    "            return jsonify({\"erro\": \"O campo 'pergunta' não pode estar vazio.\"}), 400\n",
    "\n",
    "        logger.info(f\"Recebida pergunta: '{query}' (Incluir contexto: {incluir_contexto_na_resposta})\")\n",
    "        \n",
    "        contexto_do_upload = \"\"\n",
    "        arquivos_utilizados_no_upload = []\n",
    "\n",
    "        # Processa o arquivo enviado, se houver\n",
    "        if uploaded_file and uploaded_file.filename:\n",
    "            filename = secure_filename(uploaded_file.filename) # Garante um nome de arquivo seguro\n",
    "            # Cria a pasta de uploads se não existir\n",
    "            os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n",
    "            save_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
    "            uploaded_file.save(save_path)\n",
    "            logger.info(f\"Arquivo '{filename}' recebido e salvo em '{save_path}'.\")\n",
    "            \n",
    "            # 1. Extrai texto do arquivo para uso imediato no contexto desta pergunta\n",
    "            logger.info(f\"Extraindo texto do arquivo '{filename}' para contexto imediato.\")\n",
    "            contexto_do_upload = extrair_texto(Path(save_path))\n",
    "            if contexto_do_upload:\n",
    "                arquivos_utilizados_no_upload.append(filename)\n",
    "                logger.info(f\"Texto extraído de '{filename}' para contexto: {len(contexto_do_upload)} caracteres.\")\n",
    "            else:\n",
    "                logger.warning(f\"Nenhum texto extraído do arquivo '{filename}' para contexto imediato.\")\n",
    "\n",
    "            # 2. Inicia a indexação do arquivo em uma thread separada (não bloqueia a resposta)\n",
    "            logger.info(f\"Iniciando a indexação de '{filename}' em segundo plano.\")\n",
    "            index_thread = threading.Thread(target=indexar_arquivo_unico, args=(save_path,))\n",
    "            index_thread.start() # A indexação ocorrerá em background\n",
    "\n",
    "        # Recupera contexto da base de dados (ChromaDB) existente\n",
    "        logger.info(f\"Recuperando contexto da base de dados para a query: '{query}'\")\n",
    "        contexto_da_base, arquivos_utilizados_na_base = recuperar_contexto(query)\n",
    "        if contexto_da_base:\n",
    "            logger.info(f\"Contexto recuperado da base: {len(contexto_da_base)} caracteres, de {len(arquivos_utilizados_na_base)} arquivos.\")\n",
    "        else:\n",
    "            logger.info(\"Nenhum contexto relevante encontrado na base de dados para esta query.\")\n",
    "\n",
    "        # Combina os contextos (upload e base)\n",
    "        contextos_combinados = []\n",
    "        if contexto_do_upload:\n",
    "            contextos_combinados.append(f\"--- CONTEXTO DO ARQUIVO '{uploaded_file.filename if uploaded_file else 'N/A'}' (Recém Enviado) ---\\n{contexto_do_upload}\")\n",
    "        if contexto_da_base:\n",
    "            contextos_combinados.append(f\"--- CONTEXTO DA BASE DE DADOS EXISTENTE ---\\n{contexto_da_base}\")\n",
    "        \n",
    "        contexto_final_para_prompt = \"\\n\\n\".join(contextos_combinados)\n",
    "        todos_arquivos_utilizados = list(set(arquivos_utilizados_no_upload + arquivos_utilizados_na_base))\n",
    "        \n",
    "        # Monta o prompt para o modelo Llama\n",
    "        prompt_final = f\"\"\"Você é um assistente de IA especializado em responder perguntas com base em documentos e informações fornecidas.\n",
    "\n",
    "Contexto dos documentos (se disponível):\n",
    "{contexto_final_para_prompt if contexto_final_para_prompt else \"Nenhum contexto específico foi encontrado para esta pergunta.\"}\n",
    "\n",
    "Pergunta do usuário: {query}\n",
    "\n",
    "Instruções para a resposta:\n",
    "- Responda de forma clara, concisa e objetiva em Português do Brasil.\n",
    "- Utilize APENAS as informações do contexto fornecido. Se o contexto do arquivo recém-enviado e o da base de dados estiverem disponíveis e forem relevantes, você pode usar ambos, mas dê preferência ou destaque a informação do arquivo recém-adicionado se houver conflito ou sobreposição.\n",
    "- Se a informação necessária para responder à pergunta não estiver no contexto, responda educadamente: \"A informação não foi encontrada nos documentos consultados.\" ou \"Com base nas informações disponíveis, não consigo responder a essa pergunta.\"\n",
    "- NÃO invente informações ou use conhecimento externo.\n",
    "- Ao final da sua resposta principal, se você utilizou informações de algum arquivo, liste os nomes dos arquivos fonte que foram efetivamente usados para formular a resposta, no formato: [Fonte(s) utilizada(s): nome_do_arquivo1.pdf; nome_do_arquivo2.txt]. Se nenhum arquivo específico do contexto foi usado, omita esta parte.\n",
    "\n",
    "Resposta:\"\"\"\n",
    "\n",
    "        logger.info(f\"Enviando prompt para o Llama. Tamanho do prompt: {len(prompt_final)} caracteres.\")\n",
    "        resposta_gerada = gerar_resposta(prompt_final)\n",
    "        \n",
    "        # Registra a auditoria da pergunta e resposta\n",
    "        registrar_auditoria(query, resposta_gerada, contexto_final_para_prompt, todos_arquivos_utilizados)\n",
    "\n",
    "        return jsonify({\n",
    "            \"pergunta\": query,\n",
    "            \"resposta\": resposta_gerada,\n",
    "            \"arquivos_consultados\": todos_arquivos_utilizados,\n",
    "            \"contexto_utilizado\": contexto_final_para_prompt if incluir_contexto_na_resposta else None # Retorna o contexto apenas se solicitado\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha crítica na API /perguntar: {e}\", exc_info=True)\n",
    "        return jsonify({\"erro\": f\"Erro interno no servidor ao processar a pergunta: {e}\"}), 500\n",
    "\n",
    "@app.route(\"/status\", methods=['GET'])\n",
    "def status_endpoint():\n",
    "    \"\"\"Endpoint para verificar o status da aplicação e do servidor Llama.\"\"\"\n",
    "    try:\n",
    "        chroma_collection_local = get_or_create_collection()\n",
    "        total_chunks_indexados = chroma_collection_local.count() if chroma_collection_local else 0\n",
    "        \n",
    "        return jsonify({\n",
    "            \"servidor_flask_online\": True,\n",
    "            \"servidor_llm_online\": is_server_running(),\n",
    "            \"diretorio_documentos_configurado\": DOCUMENTS_DIR,\n",
    "            \"diretorio_documentos_existe\": os.path.exists(DOCUMENTS_DIR) and os.path.isdir(DOCUMENTS_DIR),\n",
    "            \"total_chunks_indexados_chromadb\": total_chunks_indexados,\n",
    "            \"colecao_chromadb_vazia\": total_chunks_indexados == 0,\n",
    "            \"modelo_embedding_carregado\": embedding_model is not None,\n",
    "            \"tesseract_configurado\": os.path.exists(pytesseract.pytesseract.tesseract_cmd) if pytesseract.pytesseract.tesseract_cmd else False\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao verificar status da aplicação: {e}\", exc_info=True)\n",
    "        return jsonify({\"erro\": f\"Erro ao verificar status: {e}\"}), 500\n",
    "\n",
    "@app.route(\"/reindexar\", methods=['POST'])\n",
    "def reindexar_endpoint():\n",
    "    \"\"\"Endpoint para acionar a reindexação completa dos documentos.\"\"\"\n",
    "    logger.info(\"Requisição de reindexação completa recebida via API.\")\n",
    "    try:\n",
    "        # Adicionar verificação de segurança aqui se necessário (ex: token de admin)\n",
    "        sucesso_reindexacao = indexar_documentos() # Chama a função principal de indexação\n",
    "        if sucesso_reindexacao:\n",
    "            return jsonify({\"mensagem\": \"Reindexação completa dos documentos concluída com sucesso!\"})\n",
    "        else:\n",
    "            return jsonify({\"erro\": \"Falha no processo de reindexação completa. Verifique os logs para mais detalhes.\"}), 500\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro crítico durante a chamada da API /reindexar: {e}\", exc_info=True)\n",
    "        return jsonify({\"erro\": f\"Erro crítico durante a reindexação via API: {e}\"}), 500\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: BLOCO PRINCIPAL DE EXECUÇÃO (MAIN) (AJUSTADO PARA COLAB) ---\n",
    "# ==============================================================================\n",
    "# Em Colab, __name__ pode não ser '__main__' quando executado como notebook.\n",
    "# A execução do servidor é iniciada explicitamente no final.\n",
    "\n",
    "# Garante que a pasta de uploads exista ao iniciar a aplicação\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "logger.info(f\"Pasta de uploads '{UPLOAD_FOLDER}' verificada/criada.\")\n",
    "\n",
    "# Inicializações que ocorreriam no if __name__ == \"__main__\" em um script .py\n",
    "init_db() # Inicializa o DB de auditoria\n",
    "get_or_create_collection() # Garante que a coleção ChromaDB esteja pronta\n",
    "\n",
    "# Verifica se o servidor Llama está rodando. Não tenta iniciar automaticamente em Colab.\n",
    "if not is_server_running():\n",
    "    logger.warning(\"Servidor Llama não está respondendo ou não foi iniciado. A geração de respostas falhará. Certifique-se de que ele está em execução e acessível em \" + LLAMA_SERVER_URL)\n",
    "else:\n",
    "    logger.info(\"Servidor Llama detectado como em execução ou URL configurada.\")\n",
    "\n",
    "logger.info(\"=== SERVIDOR RAG PRONTO PARA INICIAR EM MODO DE APLICAÇÃO (COLAB) ===\")\n",
    "logger.info(\"O servidor Flask estará acessível através do link ngrok que será exibido ao executar app.run().\")\n",
    "logger.info(f\"Para (re)indexar todos os documentos, use a célula de indexação manual abaixo.\")\n",
    "\n",
    "# A execução do app Flask é geralmente a última coisa a ser feita em uma célula para que o ngrok funcione corretamente.\n",
    "# app.run() # Será chamado em uma célula separada para iniciar o servidor via ngrok.\n",
    "\n",
    "\n",
    "    # Verifica se o script foi chamado com o argumento \"indexar\"\n",
    "    # Em Colab, o uso de sys.argv é diferente. A indexação manual é preferida através de uma célula dedicada.\n",
    "    # A lógica original baseada em sys.argv foi removida/comentada.\n",
    "    # if len(sys.argv) > 1 and sys.argv[1].lower() == \"indexar\":\n",
    "    #    logger.info(\"=== MODO DE INDEXAÇÃO INICIADO VIA LINHA DE COMANDO ===\")\n",
    "    #    init_db() \n",
    "    #    sucesso_idx = indexar_documentos() \n",
    "    #    if sucesso_idx:\n",
    "    #        logger.info(\"\\n=== INDEXAÇÃO CONCLUÍDA COM SUCESSO (VIA LINHA DE COMANDO) ===\")\n",
    "    #        # sys.exit(0) # Removido para Colab\n",
    "    #    else:\n",
    "    #        logger.error(\"\\n=== INDEXAÇÃO FALHOU (VIA LINHA DE COMANDO) ===\")\n",
    "    #        # sys.exit(1) # Removido para Colab\n",
    "    # else:\n",
    "        # Modo normal de execução (servidor Flask)\n",
    "        # init_db() # Movido para cima\n",
    "        # get_or_create_collection() # Movido para cima\n",
    "        \n",
    "        # # Tenta iniciar o servidor Llama. Se falhar, start_llama_server já trata o sys.exit.\n",
    "        # start_llama_server() # Comentado para Colab\n",
    "        \n",
    "        # logger.info(\"=== SERVIDOR RAG INICIADO EM MODO DE APLICAÇÃO ===\")\n",
    "        # logger.info(f\"Servidor Flask estará disponível em http://{'0.0.0.0'}:5000\") # Irrelevante com ngrok\n",
    "        # logger.info(f\"Para reindexar todos os documentos, execute: python {'rag_app_colab.ipynb'} indexar\") # Instrução ajustada\n",
    "        # # Executa a aplicação Flask (debug=False para produção/uso normal)\n",
    "        # app.run(host='0.0.0.0', port=5000, debug=False) # Substituído por app.run() com ngrok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula de Dependências e Configuração Inicial\n",
    "\n",
    "Execute esta célula primeiro para instalar todas as bibliotecas necessárias e configurar o ambiente para o Tesseract OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flask-ngrok pyngrok sentence-transformers torch chromadb PyMuPDF python-docx2python Pillow pytesseract opencv-python-headless langchain requests flask-cors werkzeug\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install tesseract-ocr tesseract-ocr-por tesseract-ocr-eng\n",
    "!sudo apt-get install libgl1-mesa-glx # Dependência para OpenCV em alguns ambientes Colab\n",
    "\n",
    "# Verifica e cria pastas necessárias (exemplo)\n",
    "required_dirs = [UPLOAD_FOLDER, DOCUMENTS_DIR, CHROMA_DB_PATH, '/content/static']\n",
    "for d in required_dirs:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "        logger.info(f\"Pasta '{d}' criada.\")\n",
    "    else:\n",
    "        logger.info(f\"Pasta '{d}' já existe.\")\n",
    "\n",
    "# Lembrete para o usuário sobre arquivos\n",
    "logger.info(f\"Lembre-se de enviar seus documentos para a pasta: {DOCUMENTS_DIR}\")\n",
    "logger.info(f\"Se for usar o frontend, coloque o index.html em /content/static\")\n",
    "logger.info(f\"Se for usar um modelo Llama localmente (não recomendado para Colab pela demanda de recursos), coloque-o em: {LLAMA_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula Principal da Aplicação RAG\n",
    "\n",
    "Esta célula contém todo o código da aplicação RAG. Após executar a célula de dependências, você pode executar esta. \n",
    "A inicialização do servidor Flask com ngrok é feita em uma célula separada mais abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "from werkzeug.utils import secure_filename\n",
    "import threading\n",
    "import fitz  # PyMuPDF\n",
    "from docx2python import docx2python\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import sys\n",
    "import subprocess\n",
    "import atexit\n",
    "import torch\n",
    "\n",
    "# Imports for Colab\n",
    "from flask_ngrok import run_with_ngrok # For running Flask with ngrok in Colab\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: CONFIGURAÇÃO DE LOGGING ---\n",
    "# ==============================================================================\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
    "# File handler para escrever logs em um arquivo\n",
    "file_handler = logging.FileHandler('rag_app_colab.log', mode='w', encoding='utf-8') # Log file name changed for Colab\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "# Stream handler para exibir logs no console\n",
    "stream_handler = logging.StreamHandler(sys.stdout) # Explicitly use sys.stdout for Colab\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: CAMINHOS E PARÂMETROS GLOBAIS (AJUSTADOS PARA COLAB) ---\n",
    "# ==============================================================================\n",
    "UPLOAD_FOLDER = 'uploads_colab' # Ajustado para Colab\n",
    "DOCUMENTS_DIR = os.getenv(\"DOCUMENTS_DIR\", \"/content/Documentos\") # Ajustado para Colab, crie esta pasta e adicione seus arquivos\n",
    "CHROMA_DB_PATH = os.getenv(\"CHROMA_DB_PATH\", \"chroma_db_colab\") # Ajustado para Colab\n",
    "CHROMA_COLLECTION_NAME = \"document_chunks\"\n",
    "DB_PATH = \"audit_colab.db\"  # Banco de dados SQLite para auditoria, ajustado para Colab\n",
    "TESSERACT_CMD_PATH = os.getenv(\"TESSERACT_CMD_PATH\", \"/usr/bin/tesseract\") # Caminho padrão do Tesseract no Colab após instalação\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: PARÂMETROS DO SERVIDOR LLAMA (AJUSTADOS PARA COLAB) ---\n",
    "# ==============================================================================\n",
    "LLAMA_MODEL_PATH = os.getenv(\"LLAMA_MODEL_PATH\", \"/content/amoral-gemma3-4B-v2-qat.Q4_K_S.gguf\") # Ajustado para Colab, adicione seu modelo aqui\n",
    "LLAMA_SERVER_URL = os.getenv(\"LLAMA_SERVER_URL\", \"http://localhost:8080/completion\") # Mantenha ou ajuste se seu servidor Llama estiver em outro URL\n",
    "LLAMA_HOST = \"127.0.0.1\" # Irrelevante se start_llama_server não for usado\n",
    "LLAMA_PORT = 8080 # Irrelevante se start_llama_server não for usado\n",
    "LLAMA_NGL = -1  # Número de camadas para descarregar na GPU (-1 para GPU se disponível, 0 para CPU)\n",
    "LLAMA_THREADS = 10\n",
    "LLAMA_THREADS_BATCH = 10\n",
    "LLAMA_BATCH_SIZE = 512\n",
    "LLAMA_CONTEXT_SIZE = 4096\n",
    "LLAMA_TEMP = 0.7\n",
    "LLAMA_TOP_K = 40\n",
    "LLAMA_TOP_P = 0.95\n",
    "LLAMA_REPEAT_PENALTY = 1.1\n",
    "llama_server_process = None  # Variável global para o processo do servidor Llama (não gerenciado ativamente em Colab)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: PARÂMETROS DE EMBEDDING E INDEXAÇÃO ---\n",
    "# ==============================================================================\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # Modelo de embedding da Sentence Transformers\n",
    "CHUNK_SIZE = 500  # Tamanho dos chunks de texto\n",
    "CHUNK_OVERLAP = 50  # Sobreposição entre chunks\n",
    "TOP_K = 3  # Número de chunks mais relevantes a serem recuperados\n",
    "MAX_WORKERS = 2 # Reduzido para Colab para evitar uso excessivo de recursos\n",
    "SUPPORTED_EXTENSIONS = {'.pdf', '.docx', '.txt', '.jpg', '.jpeg', '.png'}\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: INICIALIZAÇÃO DE APLICAÇÃO E MODELOS ---\n",
    "# ==============================================================================\n",
    "app = Flask(__name__, static_folder=\"/content/static\") # Ajustado para Colab, crie /content/static e adicione index.html se necessário\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "CORS(app)  # Habilita CORS para a aplicação Flask\n",
    "run_with_ngrok(app) # Adicionado para expor o Flask app via ngrok em Colab\n",
    "\n",
    "# Define o dispositivo para o modelo de embedding (GPU se disponível, senão CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "logger.info(f\"Usando dispositivo para embedding: {device.upper()}\")\n",
    "\n",
    "logger.info(\"Carregando o modelo de embedding...\")\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=device)\n",
    "logger.info(\"Modelo de embedding carregado com sucesso.\")\n",
    "\n",
    "# Configura o caminho do Tesseract OCR, se existir\n",
    "if os.path.exists(TESSERACT_CMD_PATH):\n",
    "    pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD_PATH\n",
    "    logger.info(f\"Tesseract OCR configurado em: {TESSERACT_CMD_PATH}\")\n",
    "else:\n",
    "    logger.error(f\"Caminho do Tesseract OCR não encontrado: {TESSERACT_CMD_PATH}. A extração de texto de imagens pode falhar. Verifique a instalação.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: GERENCIAMENTO DO SERVIDOR LLAMA (AJUSTADO PARA COLAB) ---\n",
    "# Em Colab, o gerenciamento direto de processos do servidor Llama como no script original é complexo.\n",
    "# Recomenda-se executar o servidor Llama (ex: llama.cpp server) em um terminal separado ou outra instância Colab\n",
    "# e garantir que LLAMA_SERVER_URL aponte para ele.\n",
    "# As funções start_llama_server e shutdown_llama_server foram comentadas.\n",
    "# ==============================================================================\n",
    "def is_server_running():\n",
    "    \"\"\"Verifica se o servidor Llama está em execução (mantido para checagem).\"\"\"\n",
    "    # Adapta a URL para checar a raiz, assumindo que /completion é um endpoint específico\n",
    "    base_url = LLAMA_SERVER_URL.split('/completion')[0] if '/completion' in LLAMA_SERVER_URL else LLAMA_SERVER_URL\n",
    "    if not base_url.endswith('/'):\n",
    "        base_url += '/'\n",
    "    try:\n",
    "        # Tenta uma requisição simples para a raiz do servidor Llama ou um endpoint de status se conhecido\n",
    "        # Alguns servidores Llama podem não ter uma raiz acessível ou responder de forma diferente.\n",
    "        # Esta é uma tentativa genérica.\n",
    "        response = requests.get(base_url, timeout=3)\n",
    "        # Verifica se o status é OK ou se há conteúdo, indicando que o servidor está ativo\n",
    "        if response.status_code == 200 or response.text: \n",
    "            logger.info(f\"Servidor Llama parece estar respondendo em {base_url}.\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.warning(f\"Servidor Llama em {base_url} respondeu com status {response.status_code}. Pode não estar totalmente operacional.\")\n",
    "            return False # Ou True, dependendo de quão estrita a verificação precisa ser\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.warning(f\"Servidor Llama não parece estar respondendo em {base_url} (ConnectionError).\")\n",
    "        return False\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.warning(f\"Timeout ao tentar conectar com o servidor Llama em {base_url}.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao verificar o servidor Llama em {base_url}: {e}\")\n",
    "        return False\n",
    "\n",
    "# def start_llama_server():\n",
    "#     \"\"\"Inicia o servidor Llama em um processo separado (COMENTADO PARA COLAB).\"\"\"\n",
    "#     logger.warning(\"A inicialização automática do servidor Llama está desabilitada em Colab. Execute-o separadamente e configure LLAMA_SERVER_URL.\")\n",
    "\n",
    "# def shutdown_llama_server():\n",
    "#     \"\"\"Encerra o processo do servidor Llama (COMENTADO PARA COLAB).\"\"\"\n",
    "#     logger.warning(\"O encerramento automático do servidor Llama está desabilitado em Colab.\")\n",
    "\n",
    "# atexit.register(shutdown_llama_server) # Removido para Colab, pois o servidor não é gerenciado aqui\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: BANCO DE DADOS (CHROMA DB E SQLITE) E AUDITORIA ---\n",
    "# ==============================================================================\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH) # Cliente persistente para o ChromaDB\n",
    "collection = None # Variável global para a coleção do ChromaDB\n",
    "\n",
    "def get_or_create_collection():\n",
    "    \"\"\"Obtém ou cria a coleção no ChromaDB com uma função de embedding customizada.\"\"\"\n",
    "    global collection\n",
    "    if collection is None:\n",
    "        class CustomEmbeddingFunction(chromadb.api.types.EmbeddingFunction):\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "            def __call__(self, texts: list[str]) -> list[list[float]]:\n",
    "                # Gera embeddings em lotes para eficiência\n",
    "                return embedding_model.encode(texts, batch_size=32, show_progress_bar=False).tolist() # Batch size reduzido para Colab\n",
    "\n",
    "        collection = client.get_or_create_collection(\n",
    "            name=CHROMA_COLLECTION_NAME,\n",
    "            embedding_function=CustomEmbeddingFunction() # Usa a função de embedding customizada\n",
    "        )\n",
    "        logger.info(f\"Coleção '{CHROMA_COLLECTION_NAME}' carregada/criada no ChromaDB em '{CHROMA_DB_PATH}'.\")\n",
    "    return collection\n",
    "\n",
    "def init_db():\n",
    "    \"\"\"Inicializa o banco de dados SQLite para auditoria.\"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS auditoria (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    pergunta TEXT,\n",
    "                    resposta TEXT,\n",
    "                    contexto TEXT,\n",
    "                    arquivos_utilizados TEXT,\n",
    "                    data TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\")\n",
    "        logger.info(f\"Banco de dados de auditoria '{DB_PATH}' inicializado com sucesso.\")\n",
    "    except sqlite3.Error as e:\n",
    "        logger.error(f\"Erro ao inicializar o banco de dados de auditoria '{DB_PATH}': {e}\")\n",
    "\n",
    "def registrar_auditoria(pergunta: str, resposta: str, contexto: str, arquivos: list[str]):\n",
    "    \"\"\"Registra uma entrada de auditoria no banco de dados SQLite.\"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            conn.execute(\n",
    "                \"INSERT INTO auditoria (pergunta, resposta, contexto, arquivos_utilizados) VALUES (?,?,?,?)\",\n",
    "                (pergunta, resposta, contexto, '; '.join(arquivos)) # Concatena a lista de arquivos em uma string\n",
    "            )\n",
    "            conn.commit()\n",
    "        logger.info(f\"Auditoria registrada para a pergunta: '{pergunta[:50]}...'\" )\n",
    "    except sqlite3.Error as e:\n",
    "        logger.error(f\"Erro ao registrar auditoria: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: PROCESSAMENTO E EXTRAÇÃO DE TEXTO DE ARQUIVOS ---\n",
    "# ==============================================================================\n",
    "def encontrar_arquivos_recursivamente(diretorio: str) -> list[Path]:\n",
    "    \"\"\"Encontra todos os arquivos com extensões suportadas em um diretório recursivamente.\"\"\"\n",
    "    arquivos_encontrados = []\n",
    "    diretorio_path = Path(diretorio)\n",
    "    if not diretorio_path.exists() or not diretorio_path.is_dir():\n",
    "        logger.error(f\"Diretório de documentos não existe ou não é um diretório: {diretorio}\")\n",
    "        return arquivos_encontrados\n",
    "    \n",
    "    logger.info(f\"Buscando arquivos recursivamente em: {diretorio}\")\n",
    "    for extensao in SUPPORTED_EXTENSIONS:\n",
    "        pattern = f\"**/*{extensao}\" # Padrão para encontrar arquivos com a extensão\n",
    "        arquivos = list(diretorio_path.rglob(pattern))\n",
    "        arquivos_encontrados.extend(arquivos)\n",
    "        if arquivos:\n",
    "            logger.info(f\"Encontrados {len(arquivos)} arquivos com extensão {extensao}\")\n",
    "            \n",
    "    arquivos_encontrados = sorted(list(set(arquivos_encontrados))) # Remove duplicatas e ordena\n",
    "    logger.info(f\"Total de arquivos únicos encontrados: {len(arquivos_encontrados)}\")\n",
    "    return arquivos_encontrados\n",
    "\n",
    "# Funções de pré-processamento de imagem para OCR\n",
    "def get_grayscale(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Converte uma imagem para escala de cinza.\"\"\"\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def deskew(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Corrige a inclinação (skew) de uma imagem.\"\"\"\n",
    "    if len(image.shape) == 3: image = get_grayscale(image) # Converte para cinza se necessário\n",
    "    coords = np.column_stack(np.where(image > 0)) # Encontra coordenadas de pixels não pretos\n",
    "    if len(coords) < 2: return image # Retorna a imagem original se não houver pixels suficientes\n",
    "    \n",
    "    angle = cv2.minAreaRect(coords)[-1] # Calcula o ângulo de inclinação\n",
    "    if angle < -45: angle = -(90 + angle)\n",
    "    else: angle = -angle\n",
    "    \n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0) # Cria a matriz de rotação\n",
    "    # Aplica a rotação para corrigir a inclinação\n",
    "    return cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "def thresholding(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Aplica binarização (thresholding) à imagem.\"\"\"\n",
    "    if len(image.shape) == 3: image = get_grayscale(image) # Converte para cinza se necessário\n",
    "    # Usa o método de Otsu para binarização automática\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "def extrair_texto(path: Path) -> str:\n",
    "    \"\"\"Extrai texto de diferentes tipos de arquivo (PDF, DOCX, TXT, Imagens).\"\"\"\n",
    "    texto_completo = \"\"\n",
    "    try:\n",
    "        suffix = path.suffix.lower() # Obtém a extensão do arquivo em minúsculas\n",
    "        \n",
    "        if suffix == \".pdf\":\n",
    "            with fitz.open(path) as doc:\n",
    "                for page_num, page in enumerate(doc):\n",
    "                    text_native = page.get_text().strip()\n",
    "                    if text_native: # Tenta extrair texto nativo primeiro\n",
    "                        texto_completo += text_native + \"\\n\"\n",
    "                    else: # Se não houver texto nativo, tenta OCR\n",
    "                        logger.info(f\"Página {page_num+1} do PDF '{path.name}' não contém texto nativo, tentando OCR.\")\n",
    "                        pix = page.get_pixmap(dpi=200) # DPI ajustado para Colab, pode ser aumentado se necessário e os recursos permitirem\n",
    "                        img_np = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.h, pix.w, pix.n)\n",
    "                        if pix.n == 4: img_np = cv2.cvtColor(img_np, cv2.COLOR_RGBA2BGR) # Converte RGBA para BGR\n",
    "                        elif pix.n == 1: img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)\n",
    "                        \n",
    "                        # Pré-processamento da imagem para OCR\n",
    "                        img_processed = get_grayscale(img_np)\n",
    "                        img_processed = thresholding(img_processed)\n",
    "                        # O deskew pode ser computacionalmente caro e nem sempre benéfico; pode ser comentado se causar problemas\n",
    "                        # img_processed = deskew(img_processed) \n",
    "                        \n",
    "                        texto_ocr = pytesseract.image_to_string(Image.fromarray(img_processed), lang='por+eng') # OCR com Português e Inglês\n",
    "                        if texto_ocr.strip():\n",
    "                            logger.info(f\"Texto extraído via OCR da página {page_num+1} do PDF '{path.name}'.\")\n",
    "                            texto_completo += texto_ocr + \"\\n\"\n",
    "                        else:\n",
    "                            logger.warning(f\"Nenhum texto extraído (OCR) da página {page_num+1} do PDF '{path.name}'.\")\n",
    "        elif suffix == \".docx\":\n",
    "            with docx2python(str(path)) as docx_content:\n",
    "                texto_completo = docx_content.text\n",
    "        elif suffix == \".txt\":\n",
    "            # Tenta diferentes encodings comuns para arquivos de texto\n",
    "            for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
    "                try:\n",
    "                    texto_completo = path.read_text(encoding=encoding)\n",
    "                    logger.info(f\"Texto extraído de '{path.name}' com encoding '{encoding}'.\")\n",
    "                    break \n",
    "                except UnicodeDecodeError:\n",
    "                    if encoding == 'cp1252': # Se falhar no último encoding, loga o erro\n",
    "                        logger.warning(f\"Não foi possível decodificar '{path.name}' com encodings testados.\")\n",
    "                    continue\n",
    "        elif suffix in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "            img = cv2.imread(str(path))\n",
    "            if img is None: \n",
    "                logger.error(f\"Não foi possível carregar a imagem: {path.name}\")\n",
    "                raise ValueError(\"Não foi possível carregar a imagem\")\n",
    "            \n",
    "            # Pré-processamento da imagem para OCR\n",
    "            img_processed = get_grayscale(img)\n",
    "            img_processed = thresholding(img_processed)\n",
    "            # img_processed = deskew(img_processed) # Pode ser comentado\n",
    "            \n",
    "            texto_completo = pytesseract.image_to_string(Image.fromarray(img_processed), lang='por+eng') # OCR\n",
    "            if not texto_completo.strip():\n",
    "                logger.warning(f\"Nenhum texto extraído (OCR) da imagem '{path.name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha ao extrair texto de {path.name}: {e}\", exc_info=True) # Loga a exceção com traceback\n",
    "        return \"\" # Retorna string vazia em caso de erro\n",
    "    \n",
    "    return texto_completo.strip()\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: INDEXAÇÃO E GERAÇÃO DE CHUNKS ---\n",
    "# ==============================================================================\n",
    "def quebrar_em_chunks(texto: str, arquivo_path: str) -> tuple[list[str], list[dict], list[str]]:\n",
    "    \"\"\"Quebra o texto em chunks menores e gera metadados e IDs para cada chunk.\"\"\"\n",
    "    if not texto.strip(): return [], [], [] # Retorna listas vazias se o texto estiver vazio\n",
    "    \n",
    "    # Usa RecursiveCharacterTextSplitter para quebrar o texto\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, length_function=len)\n",
    "    chunks_text = splitter.split_text(texto)\n",
    "    \n",
    "    metadados_list, ids_list = [], []\n",
    "    # Normaliza o caminho do arquivo para criar IDs únicos e consistentes\n",
    "    normalized_file_path = str(Path(arquivo_path)).encode('utf-8').hex() \n",
    "    \n",
    "    for i, chunk_content in enumerate(chunks_text):\n",
    "        unique_chunk_id = f\"{normalized_file_path}_chunk_{i}\" # Cria um ID único para o chunk\n",
    "        ids_list.append(unique_chunk_id)\n",
    "        metadados_list.append({\n",
    "            'arquivo_path': str(arquivo_path), \n",
    "            'arquivo_nome': Path(arquivo_path).name, \n",
    "            'chunk_id': i\n",
    "        })\n",
    "    return chunks_text, metadados_list, ids_list\n",
    "\n",
    "def processar_arquivo_para_indexacao(arquivo_path: Path) -> tuple[list[str] | None, list[dict] | None, list[str] | None, bool]:\n",
    "    \"\"\"Processa um único arquivo: extrai texto, quebra em chunks e prepara para indexação.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Processando arquivo para indexação: {arquivo_path.name}\")\n",
    "        texto = extrair_texto(arquivo_path)\n",
    "        if not texto:\n",
    "            logger.warning(f\"Nenhum texto válido extraído de: {arquivo_path.name}. Arquivo não será indexado.\")\n",
    "            return None, None, None, False\n",
    "            \n",
    "        chunks, metadatas, ids = quebrar_em_chunks(texto, str(arquivo_path))\n",
    "        if not chunks:\n",
    "            logger.warning(f\"Texto extraído de '{arquivo_path.name}' não gerou chunks válidos. Arquivo não será indexado.\")\n",
    "            return None, None, None, False\n",
    "            \n",
    "        logger.info(f\"[{len(chunks)}] chunks gerados com sucesso de {arquivo_path.name}\")\n",
    "        return chunks, metadatas, ids, True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro crítico ao processar {arquivo_path.name} para indexação: {e}\", exc_info=True)\n",
    "        return None, None, None, False\n",
    "\n",
    "def indexar_documentos(diretorio_documentos: str = DOCUMENTS_DIR) -> bool:\n",
    "    \"\"\"Indexa todos os documentos encontrados no diretório especificado.\"\"\"\n",
    "    logger.info(f\"Iniciando processo de indexação completa dos documentos em: {diretorio_documentos}\")\n",
    "    arquivos_para_indexar = encontrar_arquivos_recursivamente(diretorio_documentos)\n",
    "    \n",
    "    if not arquivos_para_indexar:\n",
    "        logger.error(\"Nenhum arquivo encontrado para indexação. Verifique o diretório e as extensões suportadas.\")\n",
    "        return False\n",
    "\n",
    "    logger.info(\"Recriando coleção no ChromaDB para garantir uma reindexação completa...\")\n",
    "    try:\n",
    "        client.delete_collection(name=CHROMA_COLLECTION_NAME) # Deleta a coleção existente para evitar duplicatas\n",
    "        logger.info(f\"Coleção antiga '{CHROMA_COLLECTION_NAME}' deletada com sucesso.\")\n",
    "    except Exception as e: # chromadb.errors.CollectionNotFoundError ou similar\n",
    "        logger.warning(f\"Não foi possível deletar a coleção '{CHROMA_COLLECTION_NAME}' (provavelmente não existia ou erro ao deletar): {e}\")\n",
    "    \n",
    "    chroma_collection = get_or_create_collection() # Cria uma nova coleção (ou obtém se já existir, embora deletada acima)\n",
    "    \n",
    "    all_chunks_list, all_metadatas_list, all_ids_list = [], [], []\n",
    "    arquivos_processados_sucesso, arquivos_processados_erro = 0, 0\n",
    "\n",
    "    # Usa ProcessPoolExecutor para processar arquivos em paralelo - pode ser intensivo para Colab\n",
    "    # Considere processamento sequencial ou ThreadPoolExecutor para tarefas I/O bound se houver problemas de recursos\n",
    "    # max_workers = 1 # Para teste sequencial\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_file = {executor.submit(processar_arquivo_para_indexacao, arq_path): arq_path for arq_path in arquivos_para_indexar}\n",
    "        for future in as_completed(future_to_file): # Processa os resultados à medida que ficam prontos\n",
    "            try:\n",
    "                chunks, metadatas, ids, sucesso = future.result()\n",
    "                if sucesso and chunks: # Verifica se o processamento foi bem-sucedido e gerou chunks\n",
    "                    all_chunks_list.extend(chunks)\n",
    "                    all_metadatas_list.extend(metadatas)\n",
    "                    all_ids_list.extend(ids)\n",
    "                    arquivos_processados_sucesso += 1\n",
    "                else:\n",
    "                    arquivos_processados_erro += 1\n",
    "            except Exception as exc:\n",
    "                arquivo_orig = future_to_file[future]\n",
    "                logger.error(f\"Falha no processamento do arquivo {arquivo_orig.name} durante a indexação em paralelo: {exc}\", exc_info=True)\n",
    "                arquivos_processados_erro += 1\n",
    "                \n",
    "    logger.info(f\"Processamento de arquivos para indexação concluído. Sucesso: {arquivos_processados_sucesso}, Erros: {arquivos_processados_erro}.\")\n",
    "\n",
    "    if not all_chunks_list:\n",
    "        logger.warning(\"Nenhum chunk válido foi gerado para indexação após o processamento de todos os arquivos.\")\n",
    "        return False\n",
    "\n",
    "    logger.info(f\"Adicionando {len(all_chunks_list)} chunks ao ChromaDB em lotes...\")\n",
    "    try:\n",
    "        # Adiciona os chunks ao ChromaDB em lotes para otimizar o desempenho\n",
    "        batch_size_chroma = 100 # Tamanho do lote reduzido para ChromaDB em Colab\n",
    "        for i in range(0, len(all_chunks_list), batch_size_chroma):\n",
    "            chroma_collection.add(\n",
    "                documents=all_chunks_list[i:i+batch_size_chroma],\n",
    "                metadatas=all_metadatas_list[i:i+batch_size_chroma],\n",
    "                ids=all_ids_list[i:i+batch_size_chroma]\n",
    "            )\n",
    "            logger.info(f\"Lote {i//batch_size_chroma + 1} de { (len(all_chunks_list) + batch_size_chroma -1) // batch_size_chroma } adicionado ao ChromaDB.\")\n",
    "        logger.info(\"Indexação de todos os documentos concluída com sucesso!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha crítica ao adicionar documentos ao ChromaDB: {e}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "def indexar_arquivo_unico(caminho_arquivo: str):\n",
    "    \"\"\"Processa e indexa um único arquivo no ChromaDB. Usado para uploads.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"[INDEXAÇÃO DE ARQUIVO ÚNICO EM BACKGROUND] Iniciando para: {caminho_arquivo}\")\n",
    "        arquivo_path = Path(caminho_arquivo)\n",
    "        \n",
    "        if not arquivo_path.exists():\n",
    "            logger.error(f\"[INDEXAÇÃO EM BACKGROUND] Arquivo não encontrado: {caminho_arquivo}\")\n",
    "            return\n",
    "\n",
    "        chunks, metadatas, ids, sucesso = processar_arquivo_para_indexacao(arquivo_path)\n",
    "\n",
    "        if not sucesso or not chunks:\n",
    "            logger.error(f\"[INDEXAÇÃO EM BACKGROUND] Falha ao processar e gerar chunks para {arquivo_path.name}. O arquivo não será indexado.\")\n",
    "            return\n",
    "\n",
    "        chroma_collection_local = get_or_create_collection() # Garante que a coleção esteja disponível\n",
    "        # Adiciona os chunks do arquivo único à coleção existente\n",
    "        chroma_collection_local.add(\n",
    "            documents=chunks,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        logger.info(f\"[INDEXAÇÃO EM BACKGROUND] Sucesso! Adicionados {len(chunks)} chunks de {arquivo_path.name} ao ChromaDB.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[INDEXAÇÃO EM BACKGROUND] Erro crítico ao indexar o arquivo '{caminho_arquivo}': {e}\", exc_info=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: RECUPERAÇÃO DE CONTEXTO E GERAÇÃO DE RESPOSTA (RAG) ---\n",
    "# ==============================================================================\n",
    "def recuperar_contexto(query: str, top_k_chunks: int = TOP_K) -> tuple[str, list[str]]:\n",
    "    \"\"\"Recupera chunks de contexto relevantes do ChromaDB com base na query.\"\"\"\n",
    "    try:\n",
    "        chroma_collection_local = get_or_create_collection()\n",
    "        if chroma_collection_local.count() == 0:\n",
    "            logger.warning(\"O índice (ChromaDB) está vazio. Execute a reindexação primeiro ou faça upload de arquivos.\")\n",
    "            return \"\", [] # Retorna contexto vazio e lista de arquivos vazia\n",
    "            \n",
    "        # Realiza a query na coleção do ChromaDB\n",
    "        results = chroma_collection_local.query(\n",
    "            query_texts=[query], \n",
    "            n_results=top_k_chunks, \n",
    "            include=['documents', 'metadatas'] # Inclui os documentos (chunks) e metadados\n",
    "        )\n",
    "        \n",
    "        chunks_com_fonte, arquivos_utilizados_set = [], set()\n",
    "        if results and results['documents'] and results['documents'][0]:\n",
    "            document_list, metadata_list = results['documents'][0], results['metadatas'][0]\n",
    "            for doc_content, meta_info in zip(document_list, metadata_list):\n",
    "                arquivo_nome_orig = meta_info.get('arquivo_nome', 'Desconhecido')\n",
    "                arquivos_utilizados_set.add(arquivo_nome_orig)\n",
    "                chunks_com_fonte.append(f\"[Fonte: {arquivo_nome_orig}]\\n{doc_content}\") # Adiciona a fonte ao chunk\n",
    "        \n",
    "        contexto_final = \"\\n---\\n\".join(chunks_com_fonte) # Une os chunks com um separador\n",
    "        return contexto_final, list(arquivos_utilizados_set)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha ao recuperar contexto do ChromaDB: {e}\", exc_info=True)\n",
    "        return f\"Erro ao recuperar contexto do ChromaDB: {e}\", [] # Retorna mensagem de erro e lista vazia\n",
    "\n",
    "def gerar_resposta(prompt_completo: str) -> str:\n",
    "    \"\"\"Envia o prompt completo para o servidor Llama e obtém a resposta.\"\"\"\n",
    "    if not is_server_running(): # Verifica se o servidor Llama está acessível\n",
    "        logger.error(\"Servidor Llama não está em execução ou acessível. Não é possível gerar resposta.\")\n",
    "        return \"Erro: O servidor de IA (Llama) não está disponível no momento. Verifique se ele está em execução e acessível através da URL configurada.\"\n",
    "\n",
    "    try:\n",
    "        payload = {\n",
    "            \"prompt\": prompt_completo, \n",
    "            \"n_predict\": 512, # Reduzido para Colab, pode ser aumentado se os recursos permitirem\n",
    "            \"temperature\": LLAMA_TEMP, \n",
    "            \"top_k\": LLAMA_TOP_K,\n",
    "            \"top_p\": LLAMA_TOP_P,\n",
    "            \"repeat_penalty\": LLAMA_REPEAT_PENALTY,\n",
    "            \"stop\": [\"\\n\\n\", \"Pergunta:\", \"Contexto:\", \"Usuário:\"] # Palavras/frases para parar a geração\n",
    "        }\n",
    "        response = requests.post(LLAMA_SERVER_URL, json=payload, timeout=60) # Timeout ajustado\n",
    "        response.raise_for_status() # Levanta uma exceção para códigos de erro HTTP (4xx ou 5xx)\n",
    "        \n",
    "        resposta_json = response.json()\n",
    "        # Verifica se a resposta contém 'content' e se não está vazia\n",
    "        if \"content\" in resposta_json and resposta_json[\"content\"].strip():\n",
    "            return resposta_json[\"content\"].strip()\n",
    "        else:\n",
    "            logger.warning(f\"Resposta do Llama não continha 'content' ou estava vazia. Payload: {payload}, Resposta: {resposta_json}\")\n",
    "            return \"O modelo de IA não forneceu uma resposta válida para esta pergunta.\"\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Timeout na comunicação com o servidor Llama em {LLAMA_SERVER_URL}.\")\n",
    "        return f\"Erro: Timeout ao tentar conectar ao servidor de IA. A solicitação demorou muito para responder.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Falha na comunicação com o servidor Llama: {e}\")\n",
    "        return f\"Erro: Não foi possível conectar ao servidor de IA em {LLAMA_SERVER_URL}. Detalhes: {e}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha inesperada ao gerar resposta: {e}\", exc_info=True)\n",
    "        return f\"Erro inesperado ao gerar a resposta: {e}\"\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: ENDPOINTS DA API FLASK ---\n",
    "# ==============================================================================\n",
    "@app.route(\"/\")\n",
    "def frontend():\n",
    "    \"\"\"Serve a página principal do frontend (index.html).\"\"\"\n",
    "    # Verifica se a pasta static e o index.html existem\n",
    "    static_dir_path = Path(app.static_folder)\n",
    "    index_html_path = static_dir_path / \"index.html\"\n",
    "    if not static_dir_path.exists() or not index_html_path.exists():\n",
    "        logger.warning(f\"Pasta static ('{app.static_folder}') ou index.html não encontrados. O frontend pode não funcionar.\")\n",
    "        return jsonify({\"info\": \"Servidor RAG está online. Frontend não encontrado.\"}), 404\n",
    "    return send_from_directory(app.static_folder, \"index.html\")\n",
    "\n",
    "@app.route(\"/perguntar\", methods=['POST'])\n",
    "def perguntar_endpoint():\n",
    "    \"\"\"Endpoint para receber perguntas, processá-las e retornar respostas.\"\"\"\n",
    "    try:\n",
    "        # Lida com 'multipart/form-data' para permitir upload de arquivos junto com a pergunta\n",
    "        query = request.form.get(\"pergunta\", \"\").strip()\n",
    "        uploaded_file = request.files.get(\"arquivo\") # 'arquivo' é o nome do campo no form-data\n",
    "        incluir_contexto_na_resposta = request.form.get(\"incluir_contexto\", 'false').lower() == 'true'\n",
    "        \n",
    "        if not query:\n",
    "            return jsonify({\"erro\": \"O campo 'pergunta' não pode estar vazio.\"}), 400\n",
    "\n",
    "        logger.info(f\"Recebida pergunta: '{query}' (Incluir contexto: {incluir_contexto_na_resposta})\")\n",
    "        \n",
    "        contexto_do_upload = \"\"\n",
    "        arquivos_utilizados_no_upload = []\n",
    "\n",
    "        # Processa o arquivo enviado, se houver\n",
    "        if uploaded_file and uploaded_file.filename:\n",
    "            filename = secure_filename(uploaded_file.filename) # Garante um nome de arquivo seguro\n",
    "            # Cria a pasta de uploads se não existir\n",
    "            os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n",
    "            save_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
    "            uploaded_file.save(save_path)\n",
    "            logger.info(f\"Arquivo '{filename}' recebido e salvo em '{save_path}'.\")\n",
    "            \n",
    "            # 1. Extrai texto do arquivo para uso imediato no contexto desta pergunta\n",
    "            logger.info(f\"Extraindo texto do arquivo '{filename}' para contexto imediato.\")\n",
    "            contexto_do_upload = extrair_texto(Path(save_path))\n",
    "            if contexto_do_upload:\n",
    "                arquivos_utilizados_no_upload.append(filename)\n",
    "                logger.info(f\"Texto extraído de '{filename}' para contexto: {len(contexto_do_upload)} caracteres.\")\n",
    "            else:\n",
    "                logger.warning(f\"Nenhum texto extraído do arquivo '{filename}' para contexto imediato.\")\n",
    "\n",
    "            # 2. Inicia a indexação do arquivo em uma thread separada (não bloqueia a resposta)\n",
    "            logger.info(f\"Iniciando a indexação de '{filename}' em segundo plano.\")\n",
    "            index_thread = threading.Thread(target=indexar_arquivo_unico, args=(save_path,))\n",
    "            index_thread.start() # A indexação ocorrerá em background\n",
    "\n",
    "        # Recupera contexto da base de dados (ChromaDB) existente\n",
    "        logger.info(f\"Recuperando contexto da base de dados para a query: '{query}'\")\n",
    "        contexto_da_base, arquivos_utilizados_na_base = recuperar_contexto(query)\n",
    "        if contexto_da_base:\n",
    "            logger.info(f\"Contexto recuperado da base: {len(contexto_da_base)} caracteres, de {len(arquivos_utilizados_na_base)} arquivos.\")\n",
    "        else:\n",
    "            logger.info(\"Nenhum contexto relevante encontrado na base de dados para esta query.\")\n",
    "\n",
    "        # Combina os contextos (upload e base)\n",
    "        contextos_combinados = []\n",
    "        if contexto_do_upload:\n",
    "            contextos_combinados.append(f\"--- CONTEXTO DO ARQUIVO '{uploaded_file.filename if uploaded_file else 'N/A'}' (Recém Enviado) ---\\n{contexto_do_upload}\")\n",
    "        if contexto_da_base:\n",
    "            contextos_combinados.append(f\"--- CONTEXTO DA BASE DE DADOS EXISTENTE ---\\n{contexto_da_base}\")\n",
    "        \n",
    "        contexto_final_para_prompt = \"\\n\\n\".join(contextos_combinados)\n",
    "        todos_arquivos_utilizados = list(set(arquivos_utilizados_no_upload + arquivos_utilizados_na_base))\n",
    "        \n",
    "        # Monta o prompt para o modelo Llama\n",
    "        prompt_final = f\"\"\"Você é um assistente de IA especializado em responder perguntas com base em documentos e informações fornecidas.\n",
    "\n",
    "Contexto dos documentos (se disponível):\n",
    "{contexto_final_para_prompt if contexto_final_para_prompt else \"Nenhum contexto específico foi encontrado para esta pergunta.\"}\n",
    "\n",
    "Pergunta do usuário: {query}\n",
    "\n",
    "Instruções para a resposta:\n",
    "- Responda de forma clara, concisa e objetiva em Português do Brasil.\n",
    "- Utilize APENAS as informações do contexto fornecido. Se o contexto do arquivo recém-enviado e o da base de dados estiverem disponíveis e forem relevantes, você pode usar ambos, mas dê preferência ou destaque a informação do arquivo recém-adicionado se houver conflito ou sobreposição.\n",
    "- Se a informação necessária para responder à pergunta não estiver no contexto, responda educadamente: \"A informação não foi encontrada nos documentos consultados.\" ou \"Com base nas informações disponíveis, não consigo responder a essa pergunta.\"\n",
    "- NÃO invente informações ou use conhecimento externo.\n",
    "- Ao final da sua resposta principal, se você utilizou informações de algum arquivo, liste os nomes dos arquivos fonte que foram efetivamente usados para formular a resposta, no formato: [Fonte(s) utilizada(s): nome_do_arquivo1.pdf; nome_do_arquivo2.txt]. Se nenhum arquivo específico do contexto foi usado, omita esta parte.\n",
    "\n",
    "Resposta:\"\"\"\n",
    "\n",
    "        logger.info(f\"Enviando prompt para o Llama. Tamanho do prompt: {len(prompt_final)} caracteres.\")\n",
    "        resposta_gerada = gerar_resposta(prompt_final)\n",
    "        \n",
    "        # Registra a auditoria da pergunta e resposta\n",
    "        registrar_auditoria(query, resposta_gerada, contexto_final_para_prompt, todos_arquivos_utilizados)\n",
    "\n",
    "        return jsonify({\n",
    "            \"pergunta\": query,\n",
    "            \"resposta\": resposta_gerada,\n",
    "            \"arquivos_consultados\": todos_arquivos_utilizados,\n",
    "            \"contexto_utilizado\": contexto_final_para_prompt if incluir_contexto_na_resposta else None # Retorna o contexto apenas se solicitado\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha crítica na API /perguntar: {e}\", exc_info=True)\n",
    "        return jsonify({\"erro\": f\"Erro interno no servidor ao processar a pergunta: {e}\"}), 500\n",
    "\n",
    "@app.route(\"/status\", methods=['GET'])\n",
    "def status_endpoint():\n",
    "    \"\"\"Endpoint para verificar o status da aplicação e do servidor Llama.\"\"\"\n",
    "    try:\n",
    "        chroma_collection_local = get_or_create_collection()\n",
    "        total_chunks_indexados = chroma_collection_local.count() if chroma_collection_local else 0\n",
    "        \n",
    "        return jsonify({\n",
    "            \"servidor_flask_online\": True,\n",
    "            \"servidor_llm_online\": is_server_running(),\n",
    "            \"diretorio_documentos_configurado\": DOCUMENTS_DIR,\n",
    "            \"diretorio_documentos_existe\": os.path.exists(DOCUMENTS_DIR) and os.path.isdir(DOCUMENTS_DIR),\n",
    "            \"total_chunks_indexados_chromadb\": total_chunks_indexados,\n",
    "            \"colecao_chromadb_vazia\": total_chunks_indexados == 0,\n",
    "            \"modelo_embedding_carregado\": embedding_model is not None,\n",
    "            \"tesseract_configurado\": os.path.exists(pytesseract.pytesseract.tesseract_cmd) if pytesseract.pytesseract.tesseract_cmd else False\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao verificar status da aplicação: {e}\", exc_info=True)\n",
    "        return jsonify({\"erro\": f\"Erro ao verificar status: {e}\"}), 500\n",
    "\n",
    "@app.route(\"/reindexar\", methods=['POST'])\n",
    "def reindexar_endpoint():\n",
    "    \"\"\"Endpoint para acionar a reindexação completa dos documentos.\"\"\"\n",
    "    logger.info(\"Requisição de reindexação completa recebida via API.\")\n",
    "    try:\n",
    "        # Adicionar verificação de segurança aqui se necessário (ex: token de admin)\n",
    "        sucesso_reindexacao = indexar_documentos() # Chama a função principal de indexação\n",
    "        if sucesso_reindexacao:\n",
    "            return jsonify({\"mensagem\": \"Reindexação completa dos documentos concluída com sucesso!\"})\n",
    "        else:\n",
    "            return jsonify({\"erro\": \"Falha no processo de reindexação completa. Verifique os logs para mais detalhes.\"}), 500\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro crítico durante a chamada da API /reindexar: {e}\", exc_info=True)\n",
    "        return jsonify({\"erro\": f\"Erro crítico durante a reindexação via API: {e}\"}), 500\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEÇÃO: BLOCO PRINCIPAL DE EXECUÇÃO (MAIN) (AJUSTADO PARA COLAB) ---\n",
    "# ==============================================================================\n",
    "# Em Colab, __name__ pode não ser '__main__' quando executado como notebook.\n",
    "# A execução do servidor é iniciada explicitamente no final.\n",
    "\n",
    "# Garante que a pasta de uploads exista ao iniciar a aplicação\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "logger.info(f\"Pasta de uploads '{UPLOAD_FOLDER}' verificada/criada.\")\n",
    "\n",
    "# Inicializações que ocorreriam no if __name__ == \"__main__\" em um script .py\n",
    "init_db() # Inicializa o DB de auditoria\n",
    "get_or_create_collection() # Garante que a coleção ChromaDB esteja pronta\n",
    "\n",
    "# Verifica se o servidor Llama está rodando. Não tenta iniciar automaticamente em Colab.\n",
    "if not is_server_running():\n",
    "    logger.warning(\"Servidor Llama não está respondendo ou não foi iniciado. A geração de respostas falhará. Certifique-se de que ele está em execução e acessível em \" + LLAMA_SERVER_URL)\n",
    "else:\n",
    "    logger.info(\"Servidor Llama detectado como em execução ou URL configurada.\")\n",
    "\n",
    "logger.info(\"=== SERVIDOR RAG PRONTO PARA INICIAR EM MODO DE APLICAÇÃO (COLAB) ===\")\n",
    "logger.info(\"O servidor Flask estará acessível através do link ngrok que será exibido ao executar app.run().\")\n",
    "logger.info(f\"Para (re)indexar todos os documentos, use a célula de indexação manual abaixo.\")\n",
    "\n",
    "# A execução do app Flask é geralmente a última coisa a ser feita em uma célula para que o ngrok funcione corretamente.\n",
    "# app.run() # Será chamado em uma célula separada para iniciar o servidor via ngrok.\n",
    "\n",
    "\n",
    "    # Verifica se o script foi chamado com o argumento \"indexar\"\n",
    "    # Em Colab, o uso de sys.argv é diferente. A indexação manual é preferida através de uma célula dedicada.\n",
    "    # A lógica original baseada em sys.argv foi removida/comentada.\n",
    "    # if len(sys.argv) > 1 and sys.argv[1].lower() == \"indexar\":\n",
    "    #    logger.info(\"=== MODO DE INDEXAÇÃO INICIADO VIA LINHA DE COMANDO ===\")\n",
    "    #    init_db() \n",
    "    #    sucesso_idx = indexar_documentos() \n",
    "    #    if sucesso_idx:\n",
    "    #        logger.info(\"\\n=== INDEXAÇÃO CONCLUÍDA COM SUCESSO (VIA LINHA DE COMANDO) ===\")\n",
    "    #        # sys.exit(0) # Removido para Colab\n",
    "    #    else:\n",
    "    #        logger.error(\"\\n=== INDEXAÇÃO FALHOU (VIA LINHA DE COMANDO) ===\")\n",
    "    #        # sys.exit(1) # Removido para Colab\n",
    "    # else:\n",
    "        # Modo normal de execução (servidor Flask)\n",
    "        # init_db() # Movido para cima\n",
    "        # get_or_create_collection() # Movido para cima\n",
    "        \n",
    "        # # Tenta iniciar o servidor Llama. Se falhar, start_llama_server já trata o sys.exit.\n",
    "        # start_llama_server() # Comentado para Colab\n",
    "        \n",
    "        # logger.info(\"=== SERVIDOR RAG INICIADO EM MODO DE APLICAÇÃO ===\")\n",
    "        # logger.info(f\"Servidor Flask estará disponível em http://{'0.0.0.0'}:5000\") # Irrelevante com ngrok\n",
    "        # logger.info(f\"Para reindexar todos os documentos, execute: python {'rag_app_colab.ipynb'} indexar\") # Instrução ajustada\n",
    "        # # Executa a aplicação Flask (debug=False para produção/uso normal)\n",
    "        # app.run(host='0.0.0.0', port=5000, debug=False) # Substituído por app.run() com ngrok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula de Indexação Manual\n",
    "\n",
    "Descomente e execute a célula de código abaixo se precisar (re)indexar os documentos. \n",
    "Certifique-se de que:\n",
    "1. A pasta `DOCUMENTS_DIR` (definida como `/content/Documentos` por padrão) existe.\n",
    "2. Seus arquivos de documentos (`.pdf`, `.docx`, `.txt`, etc.) estão dentro desta pasta.\n",
    "3. Se estiver usando OCR para PDFs digitalizados ou imagens, o Tesseract OCR e seus pacotes de idioma (por+eng) foram instalados corretamente (ver célula de dependências)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente as linhas abaixo para executar a indexação:\n",
    "\n",
    "# logger.info(f\"Verificando DOCUMENTS_DIR: {DOCUMENTS_DIR}\")\n",
    "# if not os.path.exists(DOCUMENTS_DIR):\n",
    "#     os.makedirs(DOCUMENTS_DIR)\n",
    "#     logger.info(f\"Pasta {DOCUMENTS_DIR} criada. Por favor, adicione seus documentos e execute esta célula novamente.\")\n",
    "# elif not any(Path(DOCUMENTS_DIR).iterdir()):\n",
    "#     logger.warning(f\"A pasta {DOCUMENTS_DIR} está vazia. Adicione seus documentos para indexação.\")\n",
    "# else:\n",
    "#     logger.info(\"Iniciando o processo de indexação manual...\")\n",
    "#     # init_db() # Já chamado no início do script principal da célula\n",
    "#     # get_or_create_collection() # Já chamado no início\n",
    "#     success = indexar_documentos(DOCUMENTS_DIR)\n",
    "#     if success:\n",
    "#         logger.info(\"INDEXAÇÃO MANUAL CONCLUÍDA COM SUCESSO!\")\n",
    "#     else:\n",
    "#         logger.error(\"FALHA NA INDEXAÇÃO MANUAL. Verifique os logs acima.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula para Iniciar o Servidor Flask com ngrok\n",
    "\n",
    "Execute a célula abaixo para iniciar o servidor Flask. Um link público gerado pelo ngrok será exibido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': # Ou simplesmente remova o if para executar em Colab\n",
    "    logger.info(\"Iniciando o servidor Flask via ngrok...\")\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
